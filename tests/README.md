# Test Suite Documentation

This directory contains comprehensive tests for the GNN Attack Path project, organized into unit and integration tests.

## Test Structure

```
tests/
├── unit/                          # Unit tests for individual components
│   ├── test_baseline_scorers.py   # Baseline scoring algorithms
│   ├── test_gnn_models.py         # GNN model components
│   ├── test_mcp_components.py     # MCP server/client components
│   ├── test_data_generation.py    # Data generation components
│   ├── test_agent_components.py   # Agent planning and remediation
│   └── test_api_endpoints.py      # API endpoint testing
├── integration/                   # Integration tests for component interactions
│   ├── test_end_to_end_workflow.py # Complete workflow testing
│   └── test_mcp_integration.py    # MCP integration testing
├── test_solution.py              # Original comprehensive test suite
├── test_comprehensive.py         # New organized test suite
├── test_runner.py                # Test runner with organization
└── README.md                     # This file
```

## Test Categories

### Unit Tests (`tests/unit/`)

Unit tests focus on testing individual components in isolation with mocked dependencies.

**Coverage:**
- **Baseline Scorers** (`test_baseline_scorers.py`): Dijkstra, PageRank, Motif, and Hybrid scoring algorithms
- **GNN Models** (`test_gnn_models.py`): EdgeEncoder and AttackPathGNN model components
- **MCP Components** (`test_mcp_components.py`): MCP server, client, and tool wrapper
- **Data Generation** (`test_data_generation.py`): Synthetic data generation components
- **Agent Components** (`test_agent_components.py`): Planner, remediator, and workflow orchestration
- **API Endpoints** (`test_api_endpoints.py`): FastAPI endpoint testing

### Integration Tests (`tests/integration/`)

Integration tests verify that components work together correctly with real data flows.

**Coverage:**
- **End-to-End Workflow** (`test_end_to_end_workflow.py`): Complete data generation to analysis pipeline
- **MCP Integration** (`test_mcp_integration.py`): MCP components working together

## Running Tests

### Using Make Commands

```bash
# Run all tests
make test

# Run only unit tests
make test-unit

# Run only integration tests
make test-integration

# Run comprehensive test suite
make test-comprehensive

# Run organized test suite
make test-runner

# Run specific module tests
make test-specific MODULE=baseline
```

### Using pytest Directly

```bash
# Run all tests
pytest tests/ -v

# Run unit tests only
pytest tests/unit/ -v

# Run integration tests only
pytest tests/integration/ -v

# Run specific test file
pytest tests/unit/test_baseline_scorers.py -v

# Run with coverage
pytest tests/ -v --cov=. --cov-report=term-missing

# Run with HTML coverage report
pytest tests/ -v --cov=. --cov-report=html
```

### Using Test Runner

```bash
# Run all tests
python tests/test_runner.py all

# Run unit tests
python tests/test_runner.py unit

# Run integration tests
python tests/test_runner.py integration

# Run specific module
python tests/test_runner.py module --module baseline
```

## Test Configuration

The test suite is configured via `pytest.ini` with the following features:

- **Coverage reporting**: HTML, XML, and terminal output
- **JUnit XML**: For CI/CD integration
- **Markers**: Categorize tests by type and component
- **Strict markers**: Ensure all markers are defined
- **Verbose output**: Detailed test execution information

## Test Markers

Tests are categorized using pytest markers:

- `@pytest.mark.unit`: Unit tests for individual components
- `@pytest.mark.integration`: Integration tests for component interactions
- `@pytest.mark.performance`: Performance and load tests
- `@pytest.mark.slow`: Tests that take longer to run
- `@pytest.mark.mcp`: Tests related to Model Context Protocol
- `@pytest.mark.gnn`: Tests related to Graph Neural Networks
- `@pytest.mark.api`: Tests for API endpoints
- `@pytest.mark.agent`: Tests for agent components
- `@pytest.mark.data`: Tests for data generation
- `@pytest.mark.scoring`: Tests for scoring algorithms

## Coverage Reports

Test coverage reports are generated in multiple formats:

- **Terminal**: `--cov-report=term-missing`
- **HTML**: `--cov-report=html:htmlcov`
- **XML**: `--cov-report=xml:coverage.xml`

Coverage reports are saved to:
- `htmlcov/`: HTML coverage reports
- `coverage.xml`: XML coverage report
- `test-results/`: JUnit XML test results

## Test Data

Tests use synthetic data generated by the `SyntheticDataGenerator` class:

- **Assets**: VMs, databases, security groups, etc.
- **Edges**: Network connections and relationships
- **Vulnerabilities**: CVE entries with CVSS scores
- **Crown Jewels**: Critical assets requiring protection

## Mocking Strategy

The test suite uses comprehensive mocking to isolate components:

- **External APIs**: OpenAI, Neo4j connections
- **File I/O**: Data loading and saving operations
- **Network calls**: MCP client-server communication
- **Async operations**: Asyncio and async/await patterns

## Performance Testing

Performance tests verify:

- **Scoring performance**: < 2 seconds for typical workloads
- **Memory usage**: < 100MB increase for large datasets
- **Concurrent operations**: Thread safety and resource management
- **Scalability**: Handling of larger datasets

## Continuous Integration

The test suite is designed for CI/CD integration:

- **JUnit XML**: Test results in standard format
- **Coverage reports**: XML format for coverage tracking
- **Exit codes**: Proper exit codes for success/failure
- **Parallel execution**: Support for parallel test execution

## Best Practices

1. **Isolation**: Unit tests should not depend on external services
2. **Mocking**: Use mocks for external dependencies
3. **Coverage**: Aim for high test coverage (>80%)
4. **Performance**: Keep unit tests fast (<1 second each)
5. **Clarity**: Use descriptive test names and docstrings
6. **Maintenance**: Keep tests up-to-date with code changes

## Troubleshooting

### Common Issues

1. **Import errors**: Ensure project root is in Python path
2. **Missing dependencies**: Install all requirements from `requirements.txt`
3. **Async test failures**: Use `@pytest.mark.asyncio` for async tests
4. **Mock failures**: Check mock setup and return values
5. **Coverage issues**: Ensure all code paths are tested

### Debug Mode

Run tests in debug mode for detailed output:

```bash
pytest tests/ -v -s --tb=long
```

### Test Discovery

Check which tests are discovered:

```bash
pytest --collect-only tests/
```
