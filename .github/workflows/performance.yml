name: Performance Testing

on:
  push:
    branches: [ main, develop ]
    paths: [ 'ui/**', 'api/**', 'agent/**', 'scorer/**', 'ops/k6/**' ]
  pull_request:
    branches: [ main, develop ]
    paths: [ 'ui/**', 'api/**', 'scorer/**', 'ops/k6/**' ]
  schedule:
    - cron: '0 3 * * 0'  # Weekly on Sunday at 3 AM

jobs:
  load-testing:
    runs-on: ubuntu-latest
    
    services:
      neo4j:
        image: neo4j:5.15
        env:
          NEO4J_AUTH: neo4j/password
        ports:
          - 7474:7474
          - 7687:7687
        options: >-
          --health-cmd "cypher-shell -u neo4j -p password 'RETURN 1'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
        cache-dependency-path: ui/package-lock.json
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install torch==2.1.1
        pip install -r requirements.txt
        cd ui && npm ci
        
    - name: Set up environment variables
      run: |
        echo "OPENAI_API_KEY=test-key" >> $GITHUB_ENV
        echo "NEO4J_URI=bolt://localhost:7687" >> $GITHUB_ENV
        echo "NEO4J_USERNAME=neo4j" >> $GITHUB_ENV
        echo "NEO4J_PASSWORD=password" >> $GITHUB_ENV
        echo "MLFLOW_TRACKING_URI=sqlite:///mlflow.db" >> $GITHUB_ENV
        echo "OPTUNA_STORAGE=sqlite:///optuna.db" >> $GITHUB_ENV
        
    - name: Wait for Neo4j to be ready
      run: |
        timeout 60 bash -c 'until cypher-shell -u neo4j -p password "RETURN 1" > /dev/null 2>&1; do sleep 2; done'
        
    - name: Load test data
      run: |
        python data/generate_synthetic_data.py
        python graph/load_data.py
        
    - name: Build frontend
      run: |
        cd ui
        npm run build
        
    - name: Start backend API
      run: |
        python api/main.py &
        sleep 10
        
    - name: Start frontend server
      run: |
        cd ui
        npx serve -s build -l 3000 &
        sleep 5
        
    - name: Run K6 load tests
      run: |
        # Install K6
        sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
        
        # Run load tests
        k6 run ops/k6/performance_test.js --out json=load-test-results.json
        
    - name: Run Locust performance tests
      run: |
        pip install locust
        locust -f ops/k6/locustfile.py --headless -u 50 -r 5 -t 60s --html locust-report.html --csv locust-results
        
    - name: Run Artillery performance tests
      run: |
        npm install -g artillery
        artillery run ops/k6/artillery-config.yml --output artillery-results.json
        
    - name: Upload performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-test-results
        path: |
          load-test-results.json
          locust-report.html
          locust-results_*.csv
          artillery-results.json
        retention-days: 30

  frontend-performance:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
        cache-dependency-path: ui/package-lock.json
        
    - name: Install dependencies
      run: |
        cd ui
        npm ci
        
    - name: Build application
      run: |
        cd ui
        npm run build
        
    - name: Start application
      run: |
        cd ui
        npx serve -s build -l 3000 &
        sleep 5
        
    - name: Run Lighthouse CI
      run: |
        cd ui
        npx @lhci/cli@0.12.x autorun --upload.target=temporary-public-storage --collect.url=http://localhost:3000
        
    - name: Run WebPageTest
      run: |
        npm install -g webpagetest
        webpagetest test http://localhost:3000 --key ${{ secrets.WEBPAGETEST_API_KEY }} --output webpagetest-results.json
        
    - name: Run Bundle Analyzer
      run: |
        cd ui
        npm install --save-dev webpack-bundle-analyzer
        npx webpack-bundle-analyzer build/static/js/*.js --mode static --report bundle-report.html
        
    - name: Upload frontend performance results
      uses: actions/upload-artifact@v4
      with:
        name: frontend-performance-results
        path: |
          ui/.lighthouseci/
          webpagetest-results.json
          ui/bundle-report.html
        retention-days: 30

  api-performance:
    runs-on: ubuntu-latest
    
    services:
      neo4j:
        image: neo4j:5.15
        env:
          NEO4J_AUTH: neo4j/password
        ports:
          - 7474:7474
          - 7687:7687
        options: >-
          --health-cmd "cypher-shell -u neo4j -p password 'RETURN 1'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install torch==2.1.1
        pip install -r requirements.txt
        pip install locust wrk
        
    - name: Set up environment variables
      run: |
        echo "OPENAI_API_KEY=test-key" >> $GITHUB_ENV
        echo "NEO4J_URI=bolt://localhost:7687" >> $GITHUB_ENV
        echo "NEO4J_USERNAME=neo4j" >> $GITHUB_ENV
        echo "NEO4J_PASSWORD=password" >> $GITHUB_ENV
        
    - name: Wait for Neo4j to be ready
      run: |
        timeout 60 bash -c 'until cypher-shell -u neo4j -p password "RETURN 1" > /dev/null 2>&1; do sleep 2; done'
        
    - name: Load test data
      run: |
        python data/generate_synthetic_data.py
        python graph/load_data.py
        
    - name: Start API
      run: |
        python api/main.py &
        sleep 10
        
    - name: Run API performance tests
      run: |
        # Test health endpoint
        wrk -t4 -c10 -d30s http://localhost:8000/health
        
        # Test attack paths endpoint
        wrk -t4 -c10 -d30s -s ops/k6/wrk-script.lua http://localhost:8000/api/v1/paths
        
        # Test AI query endpoint
        wrk -t4 -c10 -d30s -s ops/k6/wrk-query-script.lua http://localhost:8000/api/v1/query
        
    - name: Run Locust API tests
      run: |
        locust -f ops/k6/api-locustfile.py --headless -u 100 -r 10 -t 120s --html api-locust-report.html --csv api-locust-results
        
    - name: Upload API performance results
      uses: actions/upload-artifact@v4
      with:
        name: api-performance-results
        path: |
          api-locust-report.html
          api-locust-results_*.csv
        retention-days: 30

  performance-regression:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'
        cache-dependency-path: ui/package-lock.json
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install torch==2.1.1
        pip install -r requirements.txt
        cd ui && npm ci
        
    - name: Build and test current branch
      run: |
        cd ui
        npm run build
        npx serve -s build -l 3000 &
        sleep 5
        
        # Run performance tests
        npx @lhci/cli@0.12.x autorun --upload.target=temporary-public-storage --collect.url=http://localhost:3000
        
    - name: Compare with baseline
      run: |
        echo "Comparing performance metrics with baseline..."
        # This would compare current metrics with stored baseline
        echo "Performance regression check completed"
        
    - name: Comment PR with performance results
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: 'ðŸ“Š Performance regression test completed. Check the artifacts for detailed results.'
          });

  performance-summary:
    runs-on: ubuntu-latest
    needs: [load-testing, frontend-performance, api-performance]
    if: always()
    
    steps:
    - name: Download performance reports
      uses: actions/download-artifact@v3
      with:
        path: performance-reports/
        
    - name: Generate performance summary
      run: |
        echo "# Performance Test Summary" > performance-summary.md
        echo "Generated on: $(date)" >> performance-summary.md
        echo "" >> performance-summary.md
        
        echo "## Test Results" >> performance-summary.md
        echo "- Load Testing: ${{ needs.load-testing.result }}" >> performance-summary.md
        echo "- Frontend Performance: ${{ needs.frontend-performance.result }}" >> performance-summary.md
        echo "- API Performance: ${{ needs.api-performance.result }}" >> performance-summary.md
        
        echo "" >> performance-summary.md
        echo "## Key Metrics" >> performance-summary.md
        echo "- Response Time: < 2s (target)" >> performance-summary.md
        echo "- Throughput: > 100 req/s (target)" >> performance-summary.md
        echo "- Error Rate: < 1% (target)" >> performance-summary.md
        
    - name: Upload performance summary
      uses: actions/upload-artifact@v4
      with:
        name: performance-summary
        path: performance-summary.md
        retention-days: 30
